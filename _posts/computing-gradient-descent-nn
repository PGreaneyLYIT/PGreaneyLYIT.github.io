---
title: 'Computing Gradient Descent in Neural Networks By Hand'
date: 2023-09-29
permalink: /posts/2023/09/computing-gradient-descent-nn/
tags:
  - gradient descent
  - neural networks
  - computational mathematics
---

**Gradient Descent in Neural Networks**

I haven't been able to find an explanation of the computation of updated network weights in a neural network with more than one node in more than one layer, which doesn't involve computing the error and jumping to define a formula using the element-wise product of vectors or matrices. 
Here we do this computation by computing the partial derivatives directly, either in vector or component form, with activation function $$\sigma(z)=\frac{1}{1+e^{-z}}.$$

```tikzpicture
\newcommand\setAngles[3]{
  \pgfmathanglebetweenpoints{\pgfpointanchor{#2}{center}}{\pgfpointanchor{#1}{center}}
  \pgfmathsetmacro\angmin{\pgfmathresult}
  \pgfmathanglebetweenpoints{\pgfpointanchor{#2}{center}}{\pgfpointanchor{#3}{center}}
  \pgfmathsetmacro\angmax{\pgfmathresult}
  \pgfmathsetmacro\dang{\angmax-\angmin}
  \pgfmathsetmacro\dang{\dang<0?\dang+360:\dang}
}
  \message{^^JNeural network with arrows}
  \readlist\Nnod{2,3,1} % array of number of nodes per layer
  
  \message{^^J  Layer}
  \foreachitem \N \in \Nnod{ % loop over layers
    \edef\lay{\Ncnt} % alias of index of current layer
    \message{\lay,}
    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
    \pgfmathsetmacro\prevt{int(\Ncnt-2)} % number of 2nd previous layer
    \node[node 1] (0) at (0,1)
      {$x_1$};
      \node[node 1] (1) at (0,0)
      {$x_2$};
    \node[node 1] (2) at (0,-1)
      {$x_3$};
    \node[node 2] (3) at (1,0.5)
      {$a_1^{(1)}$};
      \node[node 2] (4) at (1,-0.5)
      {$a_2^{(1)}$};
      \node[node 3] (5) at (2,0)
      {$\hat{y}$};
    \draw[connect arrow] (0) -- (3) node[pos=0.55] {\contour{white}{$w_{1,1}^{(1)}$}};
    \draw[connect arrow] (0) -- (4) node[pos=0.25] {\contour{white}{$w_{1,2}^{(1)}$}};
    \draw[connect arrow] (1) -- (3) node[pos=0.70] {\contour{white}{$w_{2,1}^{(1)}$}};
    \draw[connect arrow] (1) -- (4) node[pos=0.70] {\contour{white}{$w_{2,2}^{(1)}$}};
    \draw[connect arrow] (2) -- (3) node[pos=0.25] {\contour{white}{$w_{3,1}^{(1)}$}};
    \draw[connect arrow] (2) -- (4) node[pos=0.55] {\contour{white}{$w_{3,2}^{(1)}$}};
    \draw[connect arrow] (3) -- (5) node[pos=0.50] {\contour{white}{$w_{1,1}^{(2)}$}};
    \draw[connect arrow] (4) -- (5) node[pos=0.50] {\contour{white}{$w_{2,1}^{(2)}$}};
  }
```
A neural network with an input layer consisting of three nodes, a hidden layer of two nodes, and an output node with one node. The values of the hidden layer $a_j^{(1)}$ are calculated by applying the sigmoid function to the product of the weights $w_{i,j}$ with the input values $x_i$.

The mean-squared error loss function for $N$ examples is $$\mathcal{L}=\frac{1}{N}\sum_{i=1}^N (y_i-\hat{y_i})^2,$$ where $\hat{y}$ is the network output of that example at the output node, and $y$ is the  target output (label) for that example.
Suppose we want to train our network to give output $y=1$ for input values $x_1=0$, $x_2=1$, $x_3=2$. For gradient descent, we need to calculate the gradient of the output with respect to the parameters $w_{i,j}^{(k)}$.


We can write the weights between layer $i$ and layer $(i-1)$ as a matrix, so that
$$
\bv{a}^{(1)}=\sigma(W^{(1)}\bv{x}), \quad \hat{y}=\sigma(W^{(2)}\bv{a}^{(1)}).
$$
For convenience later, we define
$$
\bv{z}^{(1)}=W^{(1)}\bv{x}, \quad z^{(2)}=W^{(2)}\bv{a}^{(1)}.
$$
In component form, the first equation here is
\begin{align*}
\left(\begin{matrix}a_1^{(1)}\\a_2^{(1)}\end{matrix}\right)
=
\sigma\left(\begin{matrix}z_1^{(1)}\\z_2^{(1)}\end{matrix}\right)
=
\sigma\left(\left(\begin{matrix}w_{1,1}&w_{2,1}&w_{3,1}\\
w_{2,1}&w_{2,2}&w_{2,3}\end{matrix}\right)
\left(\begin{matrix}x_1\\x_2\\x_3\end{matrix}\right)\right)
\end{align*}
The computation of the second equation follows a similar process.

Our overall goal is to \emph{train} our network to make good predictions $\hat{y}$, by finding model parameters (weights) that minimise a loss function for inputs $x$ and labels $y$.
We do this by initialising the weights to some random values, and then using gradient descent to improve them:
$$W^{(i)}\to W^{(i)}-\frac{\partial \mathcal{L}}{\partial W^{(i)}}.$$
To initialise the network, take $$W^{(1)}=\left(\begin{matrix}0.4&0.5&0.3\\0.2&0.7&0.1\end{matrix}\right),\quad W^{(2)}=\left(\begin{matrix}0.1&0.2\end{matrix}\right),$$
$x_1=0$, $x_2=1$, $x_3=2$, and set the target output to $y=1$ for this example.
%Using the sigmoid function as the activation function throughout, 

The first step is to calculate output values at each node in the hidden layer and at the output $\hat{y}$ for the given input values.

We first need to calculate the intermediate values, 
$$ \bv{z}^{(1)}=W^{(1)}\bv{x},\, \bv{a}^{(1)}=\sigma(\bv{z}^{(1)})=\sigma\left(W^{(1)}\bv{x}\right), \text{ and } \bv{z}^{(2)}=W^{(2)}\bv{a}^{(1)},$$
and the output value $\hat{y}=\sigma(\bv{z}^{(2)}).%=W^{(2)}\bv{a}^{(1)})=%\sigma\left(W^{(2)}\sigma\left(W^{(1)}\bv{x}\right)\right).$

We want to compute the gradient of
\begin{align*}
\mathcal{L}&=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2\\
&=\frac{1}{N}\left(\bv{y}-\hat{\bv{y}}\right)^T\left(\bv{y}-\hat{\bv{y}}\right)
%&=\frac{1}{N}(\bv{y}^T\bv{y}-\bv{y}^T\hat{\bv{y}}-\hat{\bv{y}}^T\bv{y}+\hat{\bv{y}}^T\hat{\bv{y}}),
\end{align*}
which in the case of one sample reduces to 
\begin{align*}
\mathcal{L}&=(y-\hat{y})^2.
\end{align*}
In order to update the weights using gradient descent, we need to calculate
$
\dfrac{\partial\mathcal{L}}{\partial W^{(2)}}\text{ and  } \dfrac{\partial\mathcal{L}}{\partial W^{(1)}}.
$
Applying the chain rule gives
$$
\frac{\partial\mathcal{L}}{\partial W^{(2)}}=\frac{\partial\mathcal{L}}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial \bv{z}^{(2)}}\frac{\partial \bv{z}^{(2)}}{\partial W^{(2)}}
$$
Now we can use the definitions of $\mathcal{L}$, $\hat{y}$, and $\bv{z}^{(2)}$ to calculate
\begin{equation*}
\frac{\partial\mathcal{L}}{\partial\hat{y}}=-2(y-\hat{y})=2(\hat{y}-y),
\end{equation*}
and since $\hat{y}=\sigma\left(W^{(2)}\bv{a}^{(1)}\right)$, $\bv{z}^{(2)}=W^{(2)}\bv{a}^{(1)}$, we have
$$\frac{\partial\hat{y}}{\bv{z}^{(2)}}= \sigma'\left(\bv{z}^{(2)}\right), \quad \frac{\partial\bv{z}^{(2)}}{\partial W^{(2)}}={\bv{a}^{(1)}}^T,$$
$$\frac{\partial\mathcal{L}}{\partial W^{(2)}} = \underbrace{2(\hat{y}-y)}_{1\times1}\underbrace{\sigma'\left(\bv{z}^{(2)}\right)}_{1\times1}\underbrace{{\bv{a}^{(1)}}^T}_{1\times 2}.$$
We have enough information to calculate numerical values now:
\begin{align*}
\frac{\partial\mathcal{L}}{\partial W^{(2)}}
&= 2(0.554-1)\times\sigma'\left(0.217\right)\times\begin{pmatrix}0.75&0.71\end{pmatrix}\\
&=-0.22\begin{pmatrix}0.75&0.71\end{pmatrix}\\
&=\begin{pmatrix}-0.165&-0.156\end{pmatrix}
\end{align*}
Then taking a step-size of $\eta=0.1$,
\begin{align*}
W^{(2)}&\to W^{(2)}-\eta \frac{\partial\mathcal{L}}{\partial W^{(2)}} \\
&=\begin{pmatrix}0.1&0.2\end{pmatrix}-0.1\begin{pmatrix}-0.165&-0.156\end{pmatrix}\\
&=\begin{pmatrix}0.1165&0.2156\end{pmatrix}
\end{align*}
These are the new weights applied to the outputs of the hidden layer to calculate the network output $\hat{y}$.
We can similarly compute
$$
\frac{\partial\mathcal{L}}{\partial W^{(1)}}= 
\underbrace{\frac{\partial\mathcal{L}}{\partial\hat{y}}}_{1\times1}
\underbrace{\frac{\partial\hat{y}}{\partial \bv{z}^{(2)}}}_{1\times1}
\underbrace{\frac{\partial{\bv{z}^{(2)}}}{\partial \bv{a}^{(1)}}}_{1\times2}
\underbrace{\frac{\partial{\bv{a}^{(1)}}}{\partial \bv{z}^{(1)}}}_{2\times2}
\underbrace{\frac{\partial \bv{z}^{(1)}}{\partial W^{(1)}}}_{2\times(2\times3)}.
$$
This will become rather unwieldy if we use the $2\times(2\times 3)$ representation of the last term, so let's compute it in component form instead. The tensor computation is given at the end for completeness.
We have
$$
\frac{\partial \mathcal{L}}{\partial W^{(1)}}=
\begin{pmatrix}
\dfrac{\partial \mathcal{L}}{\partial w_{1,1}}
&\dfrac{\partial \mathcal{L}}{\partial w_{2,1}}
&\dfrac{\partial \mathcal{L}}{\partial w_{3,1}}\vspace*{5pt}\\
\dfrac{\partial \mathcal{L}}{\partial w_{2,1}}
&\dfrac{\partial \mathcal{L}}{\partial w_{2,2}}
&\dfrac{\partial \mathcal{L}}{\partial w_{2,3}}
\end{pmatrix},
$$
so let's calculate each of these six components.
We have 
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{1,1}}&=
\frac{\partial\mathcal{L}}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial z^{(2)}}
\left(
\frac{\partial{z^{(2)}}}{\partial a_1^{(1)}}
\frac{\partial{a_1^{(1)}}}{\partial z_1^{(1)}}
\frac{\partial z_1^{(1)}}{\partial w_{1,1}^{(1)}}
+
\frac{\partial{z^{(2)}}}{\partial a_2^{(1)}}
\frac{\partial{a_2^{(1)}}}{\partial z_2^{(1)}}
\frac{\partial z_2^{(1)}}{\partial w_{1,1}^{(1)}}
\right)\\
&=2(\hat{y}-y)\sigma'\left(z^{(2)}\right)w_{1,1}^{(2)}\sigma'\left(z_1^{(1)}\right)x_1.
\end{align*}
The full matrix is then
$$
\frac{\partial \mathcal{L}}{\partial W^{(1)}}=
2(\hat{y}-y)\sigma'\left(z^{(2)}\right)
\begin{pmatrix}
w_{1,1}^{(2)}\sigma'(z_1^{(1)})x_1
&w_{1,1}^{(2)}\sigma'(z_1^{(1)})x_2
&w_{1,1}^{(2)}\sigma'(z_1^{(1)})x_1
\vspace*{5pt}\\
w_{2,1}^{(2)}\sigma'(z_2^{(1)})x_1
&w_{2,1}^{(2)}\sigma'(z_2^{(1)})x_2
&w_{2,1}^{(2)}\sigma'(z_2^{(1)})x_3
\end{pmatrix},

We found earlier that $$\sigma'(\bv{z}^{(2)})=\sigma'(2.17)=0.092,$$ and
$$\bv{z}^{(1)}=\begin{pmatrix}1.1\\ 0.9\end{pmatrix}$$
so we calculate
\begin{align*}
\sigma'(\bv{z}^{(1)})=\sigma\begin{pmatrix}1.1\\ 0.9\end{pmatrix}\left(1-\sigma\begin{pmatrix}1.1\\ 0.9\end{pmatrix}\right)=\begin{pmatrix}0.187\\0.206\end{pmatrix}.
\end{align*}
Substituting gives
\begin{align*}
\frac{\partial\mathcal{L}}{\partial W^{(1)}}
&=-2(0.554-1)(0.092)
\begin{pmatrix}
0.187(0.1)(0)
&0.187(0.1)(1)
&0.187(0.1)(2)\\
0.206(0.2)(0)
&0.206(0.2)(1)
&0.206(0.2)(2)
\end{pmatrix},
\end{align*}


\begin{align*}
\frac{\partial\mathcal{L}}{\partial W^{(1)}}
&=0.082
\begin{pmatrix}
0
&0.0187
&0.0374\\
0
&0.0412
&0.0824
\end{pmatrix}=\begin{pmatrix}
0&0.0015&0.0031\\
0&0.0034&0.0068
\end{pmatrix}.
\end{align*}
Our updated weights after one pass are then
\begin{align*}
W^{(1)} &\to W^{(1)}-\eta\frac{\partial\mathcal{L}}{\partial W^{(1)}}\\
&=\begin{pmatrix}0.4&0.5&0.3\\0.2&0.7&0.1\end{pmatrix}-0.1\begin{pmatrix}
0&0.0015&0.0031\\
0&0.0034&0.0068
\end{pmatrix}\\
&=\begin{pmatrix}
0.4&0.4999&0.2997\\
0.2&0.6997&0.0993
\end{pmatrix}
\end{align*}
Remember that we're trying to train our network to predict $\hat{y}=1$ when $\bv{x}=(0,1,2)$.
If we're making progress, our new weights should give a better prediction than $\hat{y}=0.5541$ found from the initial set of weights.
We have
$$
\hspace*{-50pt}
\bv{z}^{(1)}=W^{(1)}\bv{x}=\begin{pmatrix}
0.4&0.4998&0.2996\\
0.2&0.6997&0.0993
\end{pmatrix}\begin{pmatrix}
0\\1\\2
\end{pmatrix}=\begin{pmatrix}
1.099\\0.8983
\end{pmatrix},
$$
$$
\bv{a}^{(1)}=\sigma(\bv{z}^{(1)})=\begin{pmatrix}
0.7501\\0.7106
\end{pmatrix},
$$
and
\begin{align*}
\bv{z}^{(2)}=W^{(2)}\bv{a}^{(1)}=\begin{pmatrix}
0.1165 &0.2156
\end{pmatrix}
\begin{pmatrix}
0.7501\\0.7106
\end{pmatrix}=0.2406
\end{align*}
Finally, our new prediction is
\begin{align*}
\hat{y}=\sigma\left(\bv{z}^{(2)}\right)=0.5599.
\end{align*}
